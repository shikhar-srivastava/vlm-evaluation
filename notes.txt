Importantly, 

### For covariance matching (whitening and coloring):

Î£_X_inv_sqrt = eigvecs_X @ torch.diag(1.0 / torch.sqrt(eigvals_X)) @ eigvecs_X.t()


### For covariance matching (+ regularization):

before inverting, add a small value to the diagonal of the covariance matrix to avoid numerical instability:

\Sigma_X \;\to\; \Sigma_X^{(\text{reg})} = \Sigma_X + \lambda \mathbf{I},

Currently, o1 suggest lambda = 1e-3.

We'll try:

## Selecting the right regularization (lambda) for L2 norm matching: 


1. 1e-3  # Norms too high (3ish range)


2. 1e-2  # Norms still high (1.3 to 1.7 range)


3. 1e-1 # Norms >0.3 to 0.8 range (lower freq. on 0.8 than textual..)


4. 5e-1 # Lower L2 Norms (less than 0.5)

# Variants to try:

1. Projector with bias that shifts the L2 norms lower:
    Something like this (per o1, which makes sense)
    ```latex
        # Suppose your final transform is z' = z * W^T + b
        # Then you want b = mu_Y - mu_X @ W^T
    ```

2. Normalizing with co-variance calculated over diverse samples of batches. 
    - For large norms: Shrinkage estimator to get the covariance matrix? 
        - Use shrinkage estimator on varying subsets (maybe Shrinkage estimator has this built in?)
        - ADD (1-lambda)*x + lambda*i, rather than just adding lambda*i.
    
    - For small norms:
        - Also, set the minimum value of the eigenvalues to a small value or minimum (1e-3).
        - Compute Covariance matrix over ATLEAST d*d samples!
        - MUST use torch.float64 (double precision) especially for the covariance and inverse. 

3. Implement the norm for models with LLM dim /= Vision dim (once normalization works in the strongest case.)

4. For Alignment training:
    - After normalization, we should see *similar distributional* activations for text vs visual embeddings. 